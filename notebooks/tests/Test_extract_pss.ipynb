{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/conda/lib/python3.7/site-packages/python_jsonschema_objects/__init__.py:53: UserWarning: Schema version http://json-schema.org/draft-04/schema not recognized. Some keywords and features may not be supported.\n",
      "  self.schema[\"$schema\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished imports\n"
     ]
    }
   ],
   "source": [
    "import meshparty\n",
    "import time\n",
    "from meshparty import trimesh_io\n",
    "import trimesh\n",
    "from trimesh.primitives import Sphere\n",
    "import os\n",
    "import h5py\n",
    "from meshparty import skeleton, skeleton_io\n",
    "import json\n",
    "import math\n",
    "import cgal_functions_Module as cfm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from scipy import sparse\n",
    "from meshparty import mesh_filters\n",
    "from meshparty import utils\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas\n",
    "import cloudvolume\n",
    "#from meshparty import skeletonize\n",
    "import pyembree\n",
    "from trimesh.ray import ray_pyembree\n",
    "from multiprocessing import Pool\n",
    "from functools import partial\n",
    "from scipy import sparse\n",
    "from meshparty import skeleton_io\n",
    "from analysisdatalink.datalink_ext import AnalysisDataLinkExt as AnalysisDataLink\n",
    "from annotationframeworkclient import infoservice\n",
    "print(\"Finished imports\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['spatial_ref_sys', 'postsynapsecompartment', 'pni_synapses_i1', 'pni_synapses_i3', 'analysisversion', 'analysistables', 'cellsegment']\n"
     ]
    }
   ],
   "source": [
    "#basil database\n",
    "\n",
    "dataset_name = 'basil'\n",
    "data_version = 1\n",
    "sqlalchemy_database_uri=\"postgresql://postgres:synapsedb@ibs-forrestc-ux1/postgres\"\n",
    "dl = AnalysisDataLink(dataset_name=dataset_name,\n",
    "                      sqlalchemy_database_uri=sqlalchemy_database_uri,\n",
    "                      verbose=False)\n",
    "print(dl.sqlalchemy_engine.table_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#params and database\n",
    "\n",
    "#basil_synapses_file = '/allen/programs/celltypes/workgroups/em-connectomics/analysis_group/basil_synapses/Basil_synapses_cleaned_final.df'\n",
    "#data = pandas.read_csv(basil_synapses_file)\n",
    "\n",
    "#for only layer 5\n",
    "#pickle_file = '/allen/programs/celltypes/workgroups/em-connectomics/analysis_group/basil_layer5_cells/Basil_layer5_neurons.pkl'\n",
    "#unpickled_df = pandas.read_pickle(pickle_file)\n",
    "#soma_ids = unpickled_df['soma_id']\n",
    "\n",
    "#for all layers:\n",
    "pickle_file = '/allen/programs/celltypes/workgroups/em-connectomics/analysis_group/basil_all_layers/Basil_all_layers_w_clusters.pkl'\n",
    "unpickled_df = pandas.read_pickle(pickle_file)\n",
    "soma_ids = unpickled_df['soma_id']\n",
    "\n",
    "\n",
    "#cone_angle = (1/4)*math.pi\n",
    "#number_of_rays = 5\n",
    "#number_of_clusters = 5\n",
    "#smoothing_lambda = 0.3\n",
    "#dist_thresh = 2500\n",
    "\n",
    "cone_angle = (1/4)*math.pi\n",
    "number_of_rays = 5\n",
    "number_of_clusters = 5\n",
    "smoothing_lambda = 0.3\n",
    "dist_thresh = 3500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EX = unpickled_df[unpickled_df['cell_type_LE']=='Neuron_Ex']\n",
    "#IN = unpickled_df[unpickled_df['cell_type_LE']=='Neuron_In']\n",
    "#NEURONS = EX.append(IN)\n",
    "#soma_ids = NEURONS['soma_id']\n",
    "\n",
    "dataset_name = 'basil'\n",
    "sqlalchemy_database_uri=\"postgresql://postgres:synapsedb@ibs-forrestc-ux1/postgres\"\n",
    "dl = AnalysisDataLink(dataset_name=dataset_name,\n",
    "                     sqlalchemy_database_uri=sqlalchemy_database_uri,\n",
    "                     materialization_version=data_version,\n",
    "                     verbose=False)\n",
    "#FUNCTIONS\n",
    "def assign_labels_to_verts(mesh,labels):\n",
    "    v = mesh.vertices\n",
    "    f = mesh.faces\n",
    "    vert_labels = np.ones(len(v))\n",
    "    for i,face in enumerate(f):\n",
    "        vert_labels[face[0]] = labels[i]\n",
    "        vert_labels[face[1]] = labels[i]\n",
    "        vert_labels[face[2]] = labels[i]\n",
    "        \n",
    "    return vert_labels\n",
    "\n",
    "def get_indices_of_skeleton_verts(mesh, sk):\n",
    "    mv = mesh.vertices\n",
    "    sv = sk.vertices\n",
    "    mvl = np.ndarray.tolist(mv)\n",
    "    indices = []\n",
    "    for s in sv:\n",
    "        if s in mv:\n",
    "            indices.append(getind(mvl, s))\n",
    "    return indices\n",
    "\n",
    "def getind(vertexlist, point):\n",
    "    for i in range(0, len(vertexlist)):\n",
    "        if( (point[0] == vertexlist[i][0]) & (point[1] == vertexlist[i][1]) & (point[2] == vertexlist[i][2])  ):\n",
    "            return i\n",
    "    return -1\n",
    "\n",
    "def create_submeshes(mesh,  labels):\n",
    "    allsubmeshes = []\n",
    "    allkeys = []\n",
    "    mydict = {}\n",
    "    unique_labels = list(set(labels))\n",
    "    for i,u in enumerate(unique_labels):\n",
    "        inds = [j for j,val in enumerate(labels) if val==u]\n",
    "        submesh = mesh.submesh([inds])\n",
    "        #print(u)\n",
    "        mydict[u] = submesh\n",
    "    return mydict\n",
    "    \n",
    "def create_closest_submesh(loc_mesh,seg,x,y,z):\n",
    "    nfaces = trimesh.proximity.nearby_faces(loc_mesh, [[x,y,z]])\n",
    "    searchseg = seg[nfaces[0][0]]\n",
    "    inds = [i for i,val in enumerate(seg) if val==searchseg]\n",
    "    spine_mesh = loc_mesh.submesh([inds]) \n",
    "    return spine_mesh[0]\n",
    "     \n",
    "def save_submeshes(submeshes,inds):\n",
    "    for i,ind in enumerate(inds):\n",
    "        trimesh.exchange.export.export_mesh(submeshes[ind][0], \"pathspine_%d_%d.off\"%(ind,i))    \n",
    "   \n",
    "def get_segments_for_synapse(d_mesh, synapse_loc):\n",
    "    x = synapse_loc[0]\n",
    "    y = synapse_loc[1]\n",
    "    z = synapse_loc[2]\n",
    "    SynSph = Sphere(center=[x,y,z],radius=100)\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    filtpts = mesh_filters.filter_spatial_distance_from_points(d_mesh, [[x,y,z]], dist_thresh)\n",
    "    #print(\"inseg 1\" , time.time() - start_time)\n",
    "\n",
    "    \n",
    "    loc_mesh = d_mesh.apply_mask(filtpts)\n",
    "    #sdf1 = calculate_sdf(loc_mesh)\n",
    "    print(loc_mesh.vertices)\n",
    "    print(loc_mesh.faces)\n",
    "    \n",
    "    sdf = cfm.cgal_sdf(loc_mesh.vertices,loc_mesh.faces, number_of_rays,cone_angle)\n",
    "    seg = cfm.cgal_segmentation(loc_mesh.vertices,loc_mesh.faces, np.asarray(sdf), number_of_clusters, smoothing_lambda)\n",
    "    \n",
    "    vertlabels = assign_labels_to_verts(loc_mesh,seg)\n",
    "    \n",
    "    #create_closest_submesh(loc_mesh,seg)\n",
    "    allmeshes = create_submeshes(loc_mesh,seg)\n",
    "    \n",
    "    return allmeshes,vertlabels,loc_mesh,[x,y,z],sdf,seg\n",
    "\n",
    "def find_path_skel2synapse_cp(loc_mesh,sk,pt):\n",
    "    t = get_indices_of_skeleton_verts(loc_mesh, sk)\n",
    "    sk_inds = [val for i,val in enumerate(t) if not val == -1 ]\n",
    "    #print(sk_inds)\n",
    "    if len(sk_inds) < 1:\n",
    "        return None\n",
    "    else:\n",
    "        closest_point, distance, tid = trimesh.proximity.closest_point(loc_mesh,[[pt[0], pt[1], pt[2] ]]) \n",
    "        print(pt)\n",
    "        print(closest_point)\n",
    "        pointindex = loc_mesh.faces[tid][0][0]\n",
    "\n",
    "        dm, preds, sources = sparse.csgraph.dijkstra(\n",
    "                        loc_mesh.csgraph, False, [pointindex], \n",
    "                        min_only=True, return_predecessors=True)\n",
    "        min_node = np.argmin(dm[sk_inds])\n",
    "\n",
    "        path = utils.get_path(pointindex, sk_inds[min_node], preds)\n",
    "        print(\"path\")\n",
    "        for p in path:\n",
    "            print(loc_mesh.vertices[p])\n",
    "        return path\n",
    "\n",
    "def find_mesh_order(path,vertlabels):\n",
    "    pathlabels = []\n",
    "    for ip in path:\n",
    "        pathlabels.append(vertlabels[ip])\n",
    "    \n",
    "    unique_path_labels = list(set(pathlabels))\n",
    "    pathlabels = list(dict.fromkeys(pathlabels)) #get ordered labels\n",
    "    #save_submeshes(allmeshes,pathlabels)\n",
    "    return pathlabels\n",
    "\n",
    "def get_indices_of_path(loc_mesh, mesh, point_inds):\n",
    "    mv =mesh.vertices\n",
    "    mvl = np.ndarray.tolist(mv)\n",
    "    indices = []\n",
    "    for p in point_inds:\n",
    "        point = loc_mesh.vertices[p]\n",
    "        if point in mv:\n",
    "            indices.append(p)\n",
    "    return indices\n",
    "\n",
    "def calculate_sdf(mesh_filt):\n",
    "    ray_inter = ray_pyembree.RayMeshIntersector(mesh_filt)\n",
    "    # The first argument below sets the origin of the ray, and I use\n",
    "    # the vertex normal to move the origin slightly so it doesn't hit at the initial vertex point\n",
    "    # and I can set multiple hits to False\n",
    "\n",
    "    # The first argument below sets the origin of the ray, and I use\n",
    "    # the vertex normal to move the origin slightly so it doesn't hit at the initial vertex point\n",
    "    # and I can set multiple hits to False\n",
    "\n",
    "    rs = np.zeros(len(mesh_filt.vertices))\n",
    "    good_rs = np.full(len(rs), False)\n",
    "\n",
    "    itr = 0\n",
    "    while not np.all(good_rs):\n",
    "        #print(np.sum(~good_rs))\n",
    "        blank_inds = np.where(~good_rs)[0]\n",
    "        starts = (mesh_filt.vertices-mesh_filt.vertex_normals)[~good_rs,:]\n",
    "        vs = (-mesh_filt.vertex_normals+0.001*np.random.rand(*mesh_filt.vertex_normals.shape))[~good_rs,:]\n",
    "\n",
    "        rtrace = ray_inter.intersects_location(starts, vs, multiple_hits=False)\n",
    "        # radius values\n",
    "        if len(rtrace[0])>0:\n",
    "            rs[blank_inds[rtrace[1]]] = np.linalg.norm(mesh_filt.vertices[rtrace[1]]-rtrace[0], axis=1)\n",
    "            good_rs[blank_inds[rtrace[1]]]=True\n",
    "        itr+=1\n",
    "        if itr>10:\n",
    "            break\n",
    "    return rs\n",
    "\n",
    "def myprocessingfunc(data_synapses,d_mesh,l,q):\n",
    "    print (\"%d out of %d \"%(q,l))\n",
    "    \n",
    "    #s = [4*data_synapses.iloc[q]['centroid_x'], 4*data_synapses.iloc[q]['centroid_y'], 40*data_synapses.iloc[q]['centroid_z']]\n",
    "    s = [4*data_synapses.iloc[q]['ctr_pt_position'][0], 4*data_synapses.iloc[q]['ctr_pt_position'][1], 40*data_synapses.iloc[q]['ctr_pt_position'][2]]\n",
    "\n",
    "    print(s)\n",
    "    allmeshes, vertlabels,loc_mesh,pt,sdf,seg = get_segments_for_synapse(d_mesh,s)\n",
    "    save_submeshes(allmeshes,range(0,len(allmeshes)))\n",
    "    time_start = time.time()\n",
    "    csg = loc_mesh._create_csgraph()\n",
    "    ccs = sparse.csgraph.connected_components(csg)\n",
    "    ccs_u, cc_sizes = np.unique(ccs[1], return_counts=True)\n",
    "    large_cc_ids = ccs_u[cc_sizes > 20]\n",
    "    etime = time.time()-time_start\n",
    "    \n",
    "    #debug\n",
    "    #trimesh.exchange.export.export_mesh(loc_mesh, \"locmesh.off\") \n",
    "    #trimesh.exchange.export.export_mesh(loc_mesh, outdir + \"/locmesh_%d.off\"%q) \n",
    "    \n",
    "    if (len(large_cc_ids) < 8) & (etime < 0.01):\n",
    "        try:\n",
    "            path = find_path_skel2synapse_cp(loc_mesh,sk,pt)\n",
    "        except:\n",
    "            path = None\n",
    "\n",
    "        if path is None:\n",
    "            spinemesh = create_closest_submesh(loc_mesh,seg,pt[0],pt[1],pt[2])\n",
    "        else:\n",
    "            start_time = time.time()\n",
    "            pathlabels = find_mesh_order(path,vertlabels)\n",
    "            \n",
    "            #debug\n",
    "            #save_submeshes(allmeshes, range(0, len(allmeshes)))\n",
    "            #save_submeshes(allmeshes,pathlabels)\n",
    "            \n",
    "            if len(pathlabels) > 1: #only look at cases where you have more than one segment (those will be either good ones or shafts)\n",
    "                sdf_verts = assign_labels_to_verts(loc_mesh,sdf)\n",
    "                sdf_mean = []\n",
    "                for ind in range(0,len(pathlabels)):\n",
    "                    lastmesh = allmeshes[pathlabels[ind]][0]\n",
    "                    t1 = get_indices_of_path(loc_mesh, lastmesh, path)\n",
    "                    sdfvec = [sdf_verts[t] for t in t1]\n",
    "                    sdf_mean.append(np.mean(sdfvec))\n",
    "\n",
    "                if sdf_mean[-1] > sdf_mean[-2]:\n",
    "                    pathlabels = pathlabels[:-1]\n",
    "            spinemeshes = [allmeshes[p] for p in pathlabels ]\n",
    "            #save_submeshes(spinemeshes,range(0,len(spinemeshes)))\n",
    "            spinemesh = trimesh.util.concatenate(spinemeshes)\n",
    "            #debug\n",
    "            #trimesh.exchange.export.export_mesh(spinemesh, \"spinemesh.off\") \n",
    "            elapsed_time = time.time() - start_time\n",
    "\n",
    "\n",
    "        trimesh.exchange.export.export_mesh(spinemesh, outdir + \"/spine_%d.off\"%q)\n",
    "        \n",
    "    \n",
    "def myParallelProcess(ahugearray, data_synapses,d_mesh):\n",
    " l = len(ahugearray)\n",
    " from multiprocessing import Pool\n",
    " from contextlib import closing\n",
    " with closing( Pool(20) ) as p:\n",
    "    partial_process = partial(myprocessingfunc,data_synapses,d_mesh,l)\n",
    "    p.map(partial_process,rng)\n",
    "    \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell id : 1861734334885\n",
      "1861734334885\n",
      "range(0, 1)\n",
      "one by one\n",
      "0\n",
      "0 out of 1 \n",
      "[372880, 200536, 9480]\n",
      "[[369417.625      200524.6875       9033.81347656]\n",
      " [369425.8125     200475.5          9128.61132812]\n",
      " [369428.90625    200765.421875     9071.4765625 ]\n",
      " ...\n",
      " [376043.53125    199966.078125    10667.58105469]\n",
      " [376048.         199904.          10760.        ]\n",
      " [376072.625      199802.328125    10663.703125  ]]\n",
      "[[    1     4     0]\n",
      " [    3     5     2]\n",
      " [    4     6     0]\n",
      " ...\n",
      " [12834 12836 12835]\n",
      " [12836 12833 12835]\n",
      " [12836 12837 12833]]\n"
     ]
    }
   ],
   "source": [
    "onebyone = True\n",
    "\n",
    "\n",
    "cellids=[1861734334885]\n",
    "\n",
    "for cell_id in cellids:\n",
    "    cell_id = int(cell_id)\n",
    "    print(\"Cell id : %d\"%cell_id)\n",
    "    outdir = '/allen/programs/celltypes/workgroups/em-connectomics/analysis_group/segmentation/synapse_based/EXPT1/%s'%str(cell_id)\n",
    "    \n",
    "    #if not os.path.exists(outdir):\n",
    "    if 1 ==1:\n",
    "        #os.makedirs(outdir)\n",
    "\n",
    "        #Download mesh and skeletonize\n",
    "\n",
    "        if not os.path.exists(outdir+'/%s_skeleton.h5'%cell_id):\n",
    "            cv_path='https://storage.googleapis.com/neuroglancer/basil_v0/basil_full/seg-aug'\n",
    "            service_endpoint = 'http://35.237.202.194/meshing/'\n",
    "            mm=trimesh_io.MeshMeta(cv_path=cv_path, disk_cache_path=outdir)\n",
    "            #this is because skeletonization was failing with saved skeletons\n",
    "            d_mesh = mm.mesh(seg_id=cell_id, merge_large_components=False,remove_duplicate_vertices=True,force_download=True)\n",
    "            #check mesh size\n",
    "            meshsize = os.path.getsize('%s/%s.h5'%(outdir,cell_id))\n",
    "            #if meshsize < 50000000:\n",
    "            if 1==1:\n",
    "                sk = skeletonize.skeletonize_mesh(d_mesh,verbose=False)\n",
    "                skeleton_io.write_skeleton_h5(sk, outdir+'/%s_skeleton.h5'%cell_id)\n",
    "\n",
    "        else:\n",
    "            cv_path='https://storage.googleapis.com/neuroglancer/basil_v0/basil_full/seg-aug'\n",
    "            service_endpoint = 'http://35.237.202.194/meshing/'\n",
    "            mm=trimesh_io.MeshMeta(cv_path=cv_path, disk_cache_path=outdir)\n",
    "            d_mesh = mm.mesh(seg_id=cell_id, merge_large_components=False)\n",
    "            sk = skeleton_io.read_skeleton_h5(outdir+'/%s_skeleton.h5'%cell_id)\n",
    "\n",
    "        \n",
    "        meshsize = os.path.getsize('%s/%s.h5'%(outdir,cell_id))\n",
    "        #if meshsize < 50000000: \n",
    "        if 1==1:\n",
    "            #find all synapses\n",
    "            #data_synapses = data.loc[data['postsyn_segid'] == cell_id]\n",
    "            data_synapses = dl.query_synapses('pni_synapses_i1', post_ids = [cell_id])\n",
    "            print(cell_id)\n",
    "\n",
    "            rng = range(0,len(data_synapses))\n",
    "            print(rng)\n",
    "            if onebyone:\n",
    "                print('one by one')\n",
    "                #for q in [pair[1]]:\n",
    "                for q in rng:\n",
    "                    print (q)\n",
    "                    myprocessingfunc(data_synapses,d_mesh,1,q)\n",
    "\n",
    "            else:\n",
    "                #multiprocessing\n",
    "                myParallelProcess(rng,data_synapses,d_mesh)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
